---
title: "Advaced R - Individual Assignment"
author: "Tomas Tello"
date: "22 de mayo de 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## House Prices Prediction Problem

This report shows the methodology followed to analyze a House Prices dataset to build a model that predicts the prices on a test set according to explanatory variables.

```{r load libraries, echo = FALSE, message=FALSE, warning=FALSE}
source('scripts/load_libraries.R')
source('scripts/f_partition.R')
source('scripts/regression_metrics.R')
```

## Data Loading

First step: Load the dataset from local storage into data table objects

```{r data loading}
raw_data_train <-fread('data/house_price_train.csv', stringsAsFactors = F)
raw_data_test <-fread('data/house_price_test.csv', stringsAsFactors = F)

str(raw_data_train)
```

## Data Cleaning and Preparation

Every transformation is declared within a function, so it can be replicated in the train and test sets.

```{r data preparation}

transformations <- function(df){
  #convert to date format
  df$date <- as.Date(df$date, "%m/%d/%Y")
  #convert discrete variables to factors
  df$zipcode <- as.factor(df$zipcode)
  #convert all integers to numeric
  df[ , names(df)[sapply(df, is.integer)]:=
             lapply(.SD,as.numeric),.SDcols = 
             names(df)[sapply(df, is.integer)]]
}

train_data <- transformations(raw_data_train)
test_data <- transformations(raw_data_test)

str(train_data)

```


## Data Exploration

Using the DataExplorer library, we can can do a quick analysis of the variables:

1) Basic metrics (including NA detection)

```{r data exploration 1}

summary(train_data)
introduce(train_data)
plot_intro(train_data)

```

2) Continuous variables analysis and how they relate to the target variable

```{r data exploration 2}

plot_density(train_data[,-c('id')])
plot_boxplot(train_data[,-c('id')], by = "price")

```

Looking at the density plots and box plots, we can conclude that the variables "view", "condition" and "floors" can be considered categorical, as they are not continuous and they show no ordinality in relation to the target variables

We then update the transformations function and apply it to the dataset:

```{r data preparation 2}

transformations <- function(df){
  #convert to date format
  df$date <- as.Date(df$date, "%m/%d/%Y")
  #convert discrete variables to factors
  df$zipcode <- as.factor(df$zipcode)
  df$condition <- as.factor(df$condition)
  df$view <- as.factor(df$view)
  df$floors <- as.factor(df$floors)
  #convert all integers to numeric
  df[ , names(df)[sapply(df, is.integer)]:=
             lapply(.SD,as.numeric),.SDcols = 
             names(df)[sapply(df, is.integer)]]
}

train_data <- transformations(raw_data_train)
test_data <- transformations(raw_data_test)

str(train_data)

```

2) Categorical variables analysis

```{r data exploration 3, message = FALSE}

plot_bar(train_data)

```

3) Correlation analysis for continuous variables

```{r data exploration 4}

plot_correlation(train_data[,-c('id')], type = "c")

```

Using the correlation matrix, we can start to do some feature selection, removing highly correlated variables. In this case: sqft_above has a 0.88 correlation with sqft_living, so we can remove this variable to avoid redundancy.

Also sqft_living15 and sqft_lot15 are highly correlated to their original counterparts, so we will remove these variables for our analysis.

```{r feature selection}
train_data_sub <- train_data[,-c('id', 'sqft_living15', 'sqft_lot15', 'sqft_above', 'date')]
test_data_sub <- test_data[,-c('id', 'sqft_living15', 'sqft_lot15', 'sqft_above', 'date')]

str(train_data_sub)
```

## Baseline Model

To run a first model (linear regression), we will first one hot encode all categorical variables. To do so, we need to stack both train and test datasets to make sure both datasets follow the same encoding.

```{r stacking datasets}
train_data_sub$train <- 1
test_data_sub$train <- 0

stacked <- rbind(train_data_sub, test_data_sub, fill = TRUE)

#Dummify the stacked data table
stacked_dum <- dummify(stacked, maxcat = 70)

#Split again based on 'train' flag
train_data_sub <- stacked_dum[stacked_dum$train == 1, -'train']
test_data_sub <- stacked_dum[stacked_dum$train == 0, -'train']

str(train_data_sub)
```

To evaluate our baseline model and the feature engineering process, we will take a holdout (validation data) from the train dataset using the f_partition script:

```{r train split}

train_val <- f_partition(train_data_sub, seed = 1414)

str(train_val)

```

Now we train our baseline model:

```{r baseline}

baseline <- lm(price ~ ., data=train_val$train)

test_lm<-predict(baseline, newdata = train_val$test)
mape_lm<-mape(real=train_val$test$price, predicted = test_lm)
mape_lm

```